\begin{frame}

\begin{itemize}
    \item Flexible Neural Machine Translation Architecture Combination
    \item Related Work
    \item Experiments
    \item \emph{\color{UOYellow}Conclusion}
\end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    \begin{itemize}
        \item Defined ADL for specifying NMT architectures on composable
            building blocks.
        \item Found RNN models benefit from multiple source attention mechanisms
            and residual feed-forward blocks.
        \item Found CNN benefits from layer normalization and feed-forward
            blocks.
        \item These features explain the effectiveness of transformers, as they
            make the respective models more transformer "like". 
        \item RNN and CNN models with self-attention on the encoder side are
            competitive with transformers. 
    \end{itemize}
\end{frame}
