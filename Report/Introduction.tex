\section{Introduction}

This paper is a response to Google Brain/Research's \textit{Attention Is All You 
Need}. If attention is all that you need, then the obvious question is "how
much?" This paper introduces the Architecture Definition Language, also known as
ADL, and then uses it to determine where attention is useful, how much is
useful, and how it compares to other network architectures. Specifically this
paper compares transformer networks to Convolutional Neural Networks, CNNs, and
Recurrent Neural Networks, RNNs. This work differentiates itself from other
works by examining specifically transformers instead of self-attention. The work
found that source attention on lower encoder layers isn't beneficial, multiple
attention and residual feed-forward layers are most beneficial, and
self-attention is most beneficial on the encoder side of the network. 
