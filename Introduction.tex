\begin{frame}
\frametitle{Overview}

Questions:
\begin{itemize}
    \item If attention is all you need, then how much?
    \item Where is the attention important?
    \item What type of attention do we need? Self? LSTM? Transformers?
\end{itemize}

\pause
Answers:
\begin{itemize}
    \item Source attention on lower encoder layers brings no additional
    benefits.
    \item Multiple source attention and residual feed-forward layers are key.
    \item Self-attention is more important for the source than for the target
    side.
\end{itemize}

\end{frame}
